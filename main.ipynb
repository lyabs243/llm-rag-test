{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ddf5f4-1415-4e5b-9483-2bae272cbc6a",
   "metadata": {},
   "source": [
    "# Lyabs journey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88499039-8b44-48d2-ae8d-a7201f5a81c1",
   "metadata": {},
   "source": [
    "\n",
    "This project aims to provide hands-on practice with Retrieval-Augmented Generation (RAG) using LLMs and LangChain.\n",
    "\n",
    "It brings together a developer's Curriculum Vitae, personal projects with descriptions, and client projects with relevant details. The data is stored in a JSON file containing paths to source files, which may be in Markdown or PDF format. All data is transformed into a vector database for use with LangChain in RAG workflows.\n",
    "\n",
    "There are three main outcomes:\n",
    "\n",
    "- The first is a chat assistant where users can ask questions about the developer or their projects.\n",
    "- The second is a fun chatbot that playfully mocks the developer's experience and projects, using humor, emojis, and punchlines.\n",
    "- The third (and perhaps most practical) is a tool to generate professional proposal texts based on client requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b033c",
   "metadata": {},
   "source": [
    "##### Read the content of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8211a21-cf5e-4f7f-8368-25f9e9dabc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read the JSON data\n",
    "with open('data/data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Process each item with a progress bar\n",
    "for item in tqdm(data, desc=\"Processing items\"):\n",
    "    print(item[\"name\"])\n",
    "\n",
    "# The total number of items\n",
    "print(f\"Total items: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb367a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some tests\n",
    "\n",
    "# Get all data where purpose is \"hackathon\"\n",
    "hackathon_data = [item for item in data if item[\"metadata\"].get(\"purpose\") == \"hackathon\"]\n",
    "print(f\"Number of hackathon items: {len(hackathon_data)}\")\n",
    "\n",
    "# The data with \"AI\" in technologies\n",
    "ai_technology_data = [item for item in data if \"AI\" in item[\"metadata\"].get(\"technologies\", [])]\n",
    "print(f\"Number of items with AI technology: {len(ai_technology_data)}\")\n",
    "\n",
    "# Print names of hackathon items\n",
    "print(\"Hackathon items:\")\n",
    "for item in hackathon_data:\n",
    "    print(f\"- {item['name']}\")\n",
    "\n",
    "# Group all hackathon items by their metadata name\n",
    "from collections import defaultdict\n",
    "hackathon_by_name = defaultdict(list)\n",
    "for item in hackathon_data:\n",
    "    hackathon_by_name[item[\"metadata\"][\"name\"]].append(item)\n",
    "\n",
    "print(\"Hackathon items grouped by name:\")\n",
    "for name, items in hackathon_by_name.items():\n",
    "    print(f\"{name}: {len(items)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deece123",
   "metadata": {},
   "source": [
    "##### Do Necessary imports for RAG\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3131ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e88b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain, plotly and Chroma\n",
    "\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.callbacks import StdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a367f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, database, and other components here\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "MODEL_4O = \"gpt-4o\"\n",
    "MODEL_5 = \"gpt-5-2025-08-07\"\n",
    "MODEL_5_MINI = \"gpt-5-mini-2025-08-07\"\n",
    "db_name = \"my_projects_vector_db\"\n",
    "dev_name = \"LoÃ¯c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c466da9",
   "metadata": {},
   "source": [
    "##### Now It's time to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f251652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each items load the documents from the 'path' key depending on the file type markdown or pdf\n",
    "def load_documents(item) -> list[Document]:\n",
    "    type = item.get(\"type\")\n",
    "    if type == \"markdown\":\n",
    "        loader = TextLoader(path)\n",
    "    elif type == \"pdf\":\n",
    "        loader = PyPDFLoader(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {path}\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Add metadata to each document\n",
    "    metadata = item.get(\"metadata\", {})\n",
    "\n",
    "    # The metadata technologies is a list, convert it to a comma-separated string\n",
    "    if \"technologies\" in metadata and isinstance(metadata[\"technologies\"], list):\n",
    "        metadata[\"technologies\"] = \", \".join(metadata[\"technologies\"])\n",
    "\n",
    "    for doc in documents:\n",
    "        doc.metadata.update(metadata)\n",
    "\n",
    "    return documents\n",
    "\n",
    "documents = []\n",
    "for item in tqdm(data, desc=\"Loading documents\", unit=\"item\"):\n",
    "    path = item.get(\"path\")\n",
    "    type = item.get(\"type\")\n",
    "    if path and type:\n",
    "        try:\n",
    "            documents.extend(load_documents(item))\n",
    "            # print(f\"Loaded {len(documents)} documents from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents from {path}: {e}\")\n",
    "    else:\n",
    "        print(f\"No valid path or type for item: {item['name']}\")\n",
    "\n",
    "print(f\"Total documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48482f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Number of document chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ca018",
   "metadata": {},
   "source": [
    "##### Let's do the embedding and store the vectors in a ChromaDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651153c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Delete existing ChromaDB database folder if it exists\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "    print(f\"Deleted existing database folder: {db_name}\")\n",
    "\n",
    "# Create and persist the ChromaDB database\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Created ChromaDB with {vectordb._collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7dc5f1",
   "metadata": {},
   "source": [
    "##### Visualize the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = vectordb._collection\n",
    "\n",
    "result = collection.get(include=[\"embeddings\", \"metadatas\", \"documents\"])\n",
    "vectors = np.array(result[\"embeddings\"])\n",
    "metadatas = result[\"metadatas\"]\n",
    "documents = result[\"documents\"]\n",
    "purpose = [meta.get(\"purpose\", \"unknown\") for meta in metadatas]\n",
    "colors = {'hackathon': 'red', 'personal': 'blue', 'community': 'green', 'unknown': 'gray', 'client': 'orange', 'event': 'purple', 'learning': 'cyan', 'codecanyon': 'brown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Creqte the 2D scatter plot\n",
    "fig = go.Figure()\n",
    "for p in set(purpose):\n",
    "    idx = [i for i, purp in enumerate(purpose) if purp == p]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=reduced_vectors[idx, 0],\n",
    "        y=reduced_vectors[idx, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(color=colors.get(p, 'black'), size=5),\n",
    "        name=p,\n",
    "        text=[f\"Doc: {documents[i][:30]}...<br>Purpose: {purpose[i]}\" for i in idx],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(title='2D Document Embeddings Visualization')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D visualization\n",
    "tsne_3d = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors_3d = tsne_3d.fit_transform(vectors)\n",
    "\n",
    "fig_3d = go.Figure()\n",
    "for p in set(purpose):\n",
    "    idx = [i for i, purp in enumerate(purpose) if purp == p]\n",
    "    fig_3d.add_trace(go.Scatter3d(\n",
    "        x=reduced_vectors_3d[idx, 0],\n",
    "        y=reduced_vectors_3d[idx, 1],\n",
    "        z=reduced_vectors_3d[idx, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(color=colors.get(p, 'black'), size=5),\n",
    "        name=p,\n",
    "        text=[f\"Doc: {documents[i][:30]}...<br>Purpose: {purpose[i]}\" for i in idx],\n",
    "        hoverinfo='text',\n",
    "    ))\n",
    "\n",
    "fig_3d.update_layout(\n",
    "    title='3D Document Embeddings Visualization',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='x', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "        yaxis=dict(title='y', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "        zaxis=dict(title='z', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "    ),\n",
    "    paper_bgcolor='#1e1e1e',  # fond principal plus doux\n",
    "    plot_bgcolor='#1e1e1e',\n",
    "    font=dict(color='white'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40),\n",
    ")\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c9a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do a chart by categories, there are 2: About Me and Project\n",
    "category = [meta.get(\"category\", \"unknown\") for meta in metadatas]\n",
    "colors_category = {'About Me': 'green', 'Project': 'magenta', 'unknown': 'gray'}\n",
    "fig_category = go.Figure()\n",
    "for c in set(category):\n",
    "    idx = [i for i, cat in enumerate(category) if cat == c]\n",
    "    fig_category.add_trace(go.Scatter(\n",
    "        x=reduced_vectors[idx, 0],\n",
    "        y=reduced_vectors[idx, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(color=colors_category.get(c, 'black'), size=5),\n",
    "        name=c,\n",
    "        text=[f\"Doc: {documents[i][:30]}...<br>Category: {category[i]}\" for i in idx],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "fig_category.update_layout(title='2D Document Embeddings by Category')\n",
    "fig_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18789e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category in 3D\n",
    "fig_category_3d = go.Figure()\n",
    "for c in set(category):\n",
    "    idx = [i for i, cat in enumerate(category) if cat == c]\n",
    "    fig_category_3d.add_trace(go.Scatter3d(\n",
    "        x=reduced_vectors_3d[idx, 0],\n",
    "        y=reduced_vectors_3d[idx, 1],\n",
    "        z=reduced_vectors_3d[idx, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(color=colors_category.get(c, 'black'), size=5),\n",
    "        name=c,\n",
    "        text=[f\"Doc: {documents[i][:30]}...<br>Category: {category[i]}\" for i in idx],\n",
    "        hoverinfo='text',\n",
    "    ))\n",
    "\n",
    "fig_category_3d.update_layout(\n",
    "    title='3D Document Embeddings by Category',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='x', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "        yaxis=dict(title='y', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "        zaxis=dict(title='z', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "    ),\n",
    "    paper_bgcolor='#1e1e1e',  # fond principal plus doux\n",
    "    plot_bgcolor='#1e1e1e',\n",
    "    font=dict(color='white'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40),\n",
    ")\n",
    "fig_category_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization by technologies (In metadata they are stored as a comma-separated string)\n",
    "technologies = []\n",
    "for meta in metadatas:\n",
    "    techs = meta.get(\"technologies\", \"\")\n",
    "    if techs:\n",
    "        technologies.extend([tech.strip() for tech in techs.split(\",\")])\n",
    "    else:\n",
    "        technologies.append(\"unknown\")\n",
    "\n",
    "# Get unique technologies and assign colors\n",
    "unique_technologies = list(set(technologies))\n",
    "color_palette = plt.get_cmap('tab20', len(unique_technologies))\n",
    "tech_colors = {tech: f'rgb({int(color_palette(i)[0]*255)}, {int(color_palette(i)[1]*255)}, {int(color_palette(i)[2]*255)})' for i, tech in enumerate(unique_technologies)}\n",
    "fig_tech = go.Figure()\n",
    "for tech in unique_technologies:\n",
    "    idx = [i for i, meta in enumerate(metadatas) if tech in meta.get(\"technologies\", \"\")]\n",
    "    if idx:\n",
    "        fig_tech.add_trace(go.Scatter(\n",
    "            x=reduced_vectors[idx, 0],\n",
    "            y=reduced_vectors[idx, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(color=tech_colors.get(tech, 'black'), size=5),\n",
    "            name=tech,\n",
    "            text=[f\"Doc: {documents[i][:30]}...<br>Technologies: {metadatas[i].get('technologies', '')}\" for i in idx],\n",
    "            hoverinfo='text'\n",
    "        ))\n",
    "\n",
    "fig_tech.update_layout(title='2D Document Embeddings by Technologies')\n",
    "fig_tech.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3D version\n",
    "fig_tech_3d = go.Figure()\n",
    "for tech in unique_technologies:\n",
    "    idx = [i for i, meta in enumerate(metadatas) if tech in meta.get(\"technologies\", \"\")]\n",
    "    if idx:\n",
    "        fig_tech_3d.add_trace(go.Scatter3d(\n",
    "            x=reduced_vectors_3d[idx, 0],\n",
    "            y=reduced_vectors_3d[idx, 1],\n",
    "            z=reduced_vectors_3d[idx, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(color=tech_colors.get(tech, 'black'), size=5),\n",
    "            name=tech,\n",
    "            text=[f\"Doc: {documents[i][:30]}...<br>Technologies: {metadatas[i].get('technologies', '')}\" for i in idx],\n",
    "            hoverinfo='text',\n",
    "        ))\n",
    "\n",
    "fig_tech_3d.update_layout(\n",
    "    title='3D Document Embeddings by Technologies',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='x', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "        yaxis=dict(title='y', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "        zaxis=dict(title='z', backgroundcolor='#1e1e1e', gridcolor='#444', zerolinecolor='#666'),\n",
    "    ),\n",
    "    paper_bgcolor='#1e1e1e',  # fond principal plus doux\n",
    "    plot_bgcolor='#1e1e1e',\n",
    "    font=dict(color='white'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40),\n",
    ")\n",
    "fig_tech_3d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b792e",
   "metadata": {},
   "source": [
    "##### Time to create the RAG chain with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "##### Time to create the RAG chain with LangChain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try simple queries\n",
    "query = \"Is there any project where I used AI?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result['answer'])\n",
    "# sources\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"- {doc.metadata.get('name', 'unknown')} ({doc.metadata.get('category', 'Unknown')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ed97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Chat interface with Gradio\n",
    "def chat_with_rag(question, chat_history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    answer = result['answer']\n",
    "    return answer\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    chat_with_rag,\n",
    "    type=\"messages\",\n",
    ")\n",
    "iface.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec2a14",
   "metadata": {},
   "source": [
    "### First project: a chat assistant about me and my projects\n",
    "\n",
    "This is a simple chat assistant where users can ask questions about me and my projects. The answers are based on the data stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", ],\n",
    "    template=f\"\"\"You are an assistant called \"LyabsInfo\", specialized in answering questions about {dev_name}'s projects and background.\n",
    "The one speaking to you is {dev_name}'s clients or potential clients. You have to look into his projects and background to answer their questions.\n",
    "\n",
    "When you don't know the answer, say so and suggest other questions about {dev_name}'s projects and background.\n",
    "\n",
    "The question to answer: {{question}}\n",
    "Here is the context (translate it to the question language) you can use to answer the question: {{context}}\n",
    "\n",
    "Answer in english or French depending on the language of the question. Do not  use 2 languages in the same answer.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')\n",
    "\n",
    "# retriever with filter on category\n",
    "retriever = vectordb.as_retriever(\n",
    "    # search_kwargs={\"filter\": {\"category\": \"Project\"}, \"k\": 10}\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    #callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question to test the new prompt\n",
    "query = \"I need a project on Firebase can he works on it?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat with Gradio\n",
    "def chat_with_rag(question, chat_history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    answer = result['answer']\n",
    "    return answer\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    chat_with_rag,\n",
    "    type=\"messages\",\n",
    ")\n",
    "iface.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c45a72",
   "metadata": {},
   "source": [
    "### Second Project: A troll chat bot for fun  ð¤ª\n",
    "An assistant that is discussing with me and respond with some punchline to mock my background and projects just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e72aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", ],\n",
    "    template=f\"\"\"You are a troll chat bot just for fun. You are discussing with {dev_name}, the one that created you. based on\n",
    " the question: {{question}} and the context: {{context}}\n",
    "Troll him with no mercy, be funny and creative. Use a lot of sarcasm. Make fun of his projects and background. Use a lot of humor with emojis.\n",
    "\n",
    "Give a short answer.\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    #callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question to test the new prompt\n",
    "query = \"Mon CV est extraordinaire n'est-ce pas? Reponds en francais stp.\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df50f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat with Gradio\n",
    "def chat_with_rag(question, chat_history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    answer = result['answer']\n",
    "    return answer\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    chat_with_rag,\n",
    "    type=\"messages\",\n",
    "    title=\"Troll Chatbot ð¤ª\"\n",
    ")\n",
    "iface.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d358b1",
   "metadata": {},
   "source": [
    "### Third project: A bid generator from offer\n",
    "Based on an offer text, generate a bid proposal text. From projects and background, the LLM will generate a professional bid proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a professional bid proposal generator. Based on the offer text, generate a bid proposal text.\n",
    "See what the offer is about and generate a professional bid as you are applying for a job as me {dev_name}, a freelance developer.\n",
    "\n",
    "Here is the offer text: {{question}}\n",
    "\n",
    "My background/projects: {{context}}\n",
    "\n",
    "The bid should be in the same language as the offer text.\n",
    "\n",
    "Do not mention information that is not in the context or the offer text. For example if the offer text is about a Flutter mobile app, do not talk about web development.\n",
    "Do not estimate any price nor time or talk about features that are not in the offer text. You can mention examples of similar projects I did in the past if relevant.\n",
    "Do not place headers or footers, just the bid text.\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=MODEL_5_MINI, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt)},\n",
    "    #callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple offer text to test the new prompt\n",
    "offer_text = \"\"\"We are seeking a skilled developer to reskin a Flutter template app for both iOS and Android platforms that we have recently purchased. The ideal candidate will not only change the design elements but also ensure the app is set up correctly for deployment. Your expertise in Flutter and mobile app development will be crucial for this project. Please provide examples of previous app reskinning work in your application\"\"\"\n",
    "result = conversation_chain.invoke({\"question\": offer_text})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI with Gradio\n",
    "def generate_bid(offer):\n",
    "    result = conversation_chain.invoke({\"question\": offer})\n",
    "    bid = result['answer']\n",
    "    return bid\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generate_bid,\n",
    "    inputs=gr.Textbox(\n",
    "        lines=10,\n",
    "        placeholder=\"Paste the offer text here...\",\n",
    "        label=\"Offer Text\",\n",
    "    ),\n",
    "    outputs=gr.Textbox(\n",
    "        lines=20,\n",
    "        placeholder=\"The generated bid proposal will appear here...\",\n",
    "        label=\"Generated Bid Proposal\",\n",
    "    ),\n",
    "    title=\"Bid Proposal Generator ð¤\",\n",
    ")\n",
    "iface.launch(inbrowser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632f389",
   "metadata": {},
   "source": [
    "#### Bid generator with option to choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rag_chain(model_name):\n",
    "    llm = ChatOpenAI(model_name=model_name, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')\n",
    "\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_kwargs={\"k\": 10}\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt)},\n",
    "        #callbacks=[StdOutCallbackHandler()],\n",
    "    )\n",
    "    return conversation_chain\n",
    "\n",
    "def generate_bid(offer, model):\n",
    "    conversation_chain = init_rag_chain(model)\n",
    "    result = conversation_chain.invoke({\"question\": offer})\n",
    "    return result['answer']\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generate_bid,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            lines=10,\n",
    "            placeholder=\"Paste the offer text here...\",\n",
    "            label=\"Offer Text\",\n",
    "        ),\n",
    "        gr.Dropdown(\n",
    "            choices=[\n",
    "                (\"GPT-4o\", MODEL_4O),\n",
    "                (\"GPT-4o Mini\", MODEL),\n",
    "                (\"GPT-5\", MODEL_5),\n",
    "                (\"GPT-5 Mini\", MODEL_5_MINI),\n",
    "            ],\n",
    "            value=MODEL_5_MINI,\n",
    "            label=\"Select Model\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Textbox(\n",
    "        lines=20,\n",
    "        placeholder=\"The generated bid proposal will appear here...\",\n",
    "        label=\"Generated Bid Proposal\",\n",
    "        show_copy_button=True,\n",
    "    ),\n",
    "    title=\"Bid Proposal Generator ð¤\",\n",
    ")\n",
    "iface.launch(inbrowser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6e0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
